#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆå®æ—¶è½¬å½•ç³»ç»Ÿ
åŸºäºsounddeviceå®˜æ–¹ç¤ºä¾‹ï¼Œåªä¿ç•™è½¬å½•åŠŸèƒ½
"""

import sounddevice as sd
import numpy as np
import queue
import threading
import time
import subprocess
import tempfile
import wave
import json
import os
import sys
import webrtcvad
from colorama import init, Fore, Style

# åˆå§‹åŒ–colorama
init(autoreset=True)


class SimpleTranscriber:
    def __init__(self, whisper_model="small"):
        print(f"{Fore.CYAN}ğŸš€ åˆå§‹åŒ–ç®€åŒ–ç‰ˆè½¬å½•ç³»ç»Ÿ{Style.RESET_ALL}")

        # éŸ³é¢‘è®¾ç½®
        self.sample_rate = None
        self.frame_duration = 30  # ms
        self.frame_size = None

        # åˆ†æ®µå‚æ•° - æé«˜è½¬å½•é¢‘ç‡
        self.min_segment_duration = 1  # ç§’ - ä»2.0é™åˆ°1.5
        self.max_segment_duration = 3.0  # ç§’ - ä»8.0é™åˆ°4.0
        self.silence_threshold = 10  # é™éŸ³å¸§æ•° - ä»50é™åˆ°20

        # è®¡ç®—å¸§æ•°
        self.min_segment_frames = None
        self.max_segment_frames = None

        # éŸ³é¢‘ç¼“å†²
        self.speech_frames = []
        self.silence_count = 0

        # é˜Ÿåˆ—å’ŒçŠ¶æ€
        self.audio_queue = queue.Queue()
        self.listening = False
        self.transcription_history = []
        self.segment_counter = 0

        # è®¾ç½®Whisper
        self.setup_whisper(whisper_model)

        # è®¾ç½®éŸ³é¢‘è®¾å¤‡
        self.setup_audio_device()

        # è®¾ç½®WebRTC VAD
        self.setup_vad()

        print(f"{Fore.GREEN}âœ“ è½¬å½•ç³»ç»Ÿå°±ç»ª{Style.RESET_ALL}")

    def setup_whisper(self, whisper_model):
        """è®¾ç½®Whisperæ¨¡å‹"""
        try:
            result = subprocess.run(
                ["../whisper.cpp/build/bin/whisper-cli", "--help"],
                capture_output=True,
                text=True,
            )
            print(f"{Fore.GREEN}âœ“ whisper-cli å¯ç”¨{Style.RESET_ALL}")

            model_configs = {
                "tiny": {"file": "ggml-tiny.bin", "size": "39MB"},
                "base": {"file": "ggml-base.bin", "size": "147MB"},
                "small": {"file": "ggml-small.bin", "size": "244MB"},
                "small.en": {"file": "ggml-small.en.bin", "size": "244MB"},
                "medium": {"file": "ggml-medium.bin", "size": "769MB"},
                "large-v3": {"file": "ggml-large-v3.bin", "size": "1.55GB"},
            }

            if whisper_model not in model_configs:
                print(f"{Fore.RED}âŒ ä¸æ”¯æŒçš„æ¨¡å‹: {whisper_model}{Style.RESET_ALL}")
                sys.exit(1)

            config = model_configs[whisper_model]
            self.whisper_model_path = f'./models/{config["file"]}'

            if not os.path.exists(self.whisper_model_path):
                print(
                    f"{Fore.RED}âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {self.whisper_model_path}{Style.RESET_ALL}"
                )
                print(
                    f"{Fore.YELLOW}è¯·ä¸‹è½½æ¨¡å‹: curl -L -o {self.whisper_model_path} https://huggingface.co/ggerganov/whisper.cpp/resolve/main/{config['file']}{Style.RESET_ALL}"
                )
                sys.exit(1)

            # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(self.whisper_model_path)
            expected_sizes = {
                "tiny": 39 * 1024 * 1024,  # 39MB
                "base": 147 * 1024 * 1024,  # 147MB
                "small": 244 * 1024 * 1024,  # 244MB
                "medium": 769 * 1024 * 1024,  # 769MB
                "large-v3": 1.55 * 1024 * 1024 * 1024,  # 1.55GB
            }

            expected_size = expected_sizes.get(whisper_model, 0)
            if file_size < expected_size * 0.9:  # å…è®¸10%çš„è¯¯å·®
                print(
                    f"{Fore.RED}âŒ æ¨¡å‹æ–‡ä»¶å¯èƒ½æŸå: {self.whisper_model_path}{Style.RESET_ALL}"
                )
                print(f"   å½“å‰å¤§å°: {file_size / (1024*1024):.1f}MB")
                print(f"   æœŸæœ›å¤§å°: {expected_size / (1024*1024):.1f}MB")
                print(f"{Fore.YELLOW}è¯·é‡æ–°ä¸‹è½½æ¨¡å‹æ–‡ä»¶{Style.RESET_ALL}")
                sys.exit(1)

            print(
                f"{Fore.CYAN}ğŸ§  ä½¿ç”¨Whisperæ¨¡å‹: {whisper_model} ({config['size']}){Style.RESET_ALL}"
            )

        except FileNotFoundError:
            print(f"{Fore.RED}âŒ whisper-cli æœªå®‰è£…{Style.RESET_ALL}")
            print(f"{Fore.YELLOW}macOSå®‰è£…: brew install whisper-cpp{Style.RESET_ALL}")
            sys.exit(1)

    def setup_audio_device(self):
        """è®¾ç½®éŸ³é¢‘è®¾å¤‡"""
        print(f"{Fore.CYAN}ğŸµ è®¾ç½®éŸ³é¢‘è®¾å¤‡...{Style.RESET_ALL}")

        # åˆ—å‡ºå¯ç”¨è®¾å¤‡
        devices = sd.query_devices()
        print(f"{Fore.YELLOW}å¯ç”¨éŸ³é¢‘è®¾å¤‡:{Style.RESET_ALL}")

        blackhole_id = None
        multi_output_id = None

        for i, device in enumerate(devices):
            device_info = f"  [{i}] {device['name']} - {device['max_input_channels']}in/{device['max_output_channels']}out"
            print(device_info)

            # æŸ¥æ‰¾BlackHoleå’ŒMulti-Output Device
            device_name = device["name"].lower()
            if "blackhole" in device_name:
                blackhole_id = i
            elif "multi" in device_name and "output" in device_name:
                multi_output_id = i

        print()

        # é€‰æ‹©BlackHoleä½œä¸ºæ•è·è®¾å¤‡
        if blackhole_id is not None:
            self.audio_device = blackhole_id
            print(
                f"{Fore.GREEN}âœ“ é€‰æ‹©BlackHoleè®¾å¤‡: [{blackhole_id}] {devices[blackhole_id]['name']}{Style.RESET_ALL}"
            )
        else:
            print(f"{Fore.RED}âŒ æœªæ‰¾åˆ°BlackHoleè®¾å¤‡{Style.RESET_ALL}")
            sys.exit(1)

        # è·å–è®¾å¤‡ä¿¡æ¯å¹¶è®¾ç½®é‡‡æ ·ç‡
        try:
            device_info = sd.query_devices(self.audio_device, "input")
            self.sample_rate = int(device_info["default_samplerate"])
            print(
                f"{Fore.GREEN}âœ“ éŸ³é¢‘è®¾å¤‡é‡‡æ ·ç‡: {self.sample_rate} Hz{Style.RESET_ALL}"
            )
        except Exception as e:
            print(
                f"{Fore.YELLOW}âš ï¸ æ— æ³•è·å–è®¾å¤‡é‡‡æ ·ç‡ï¼Œä½¿ç”¨é»˜è®¤å€¼: {e}{Style.RESET_ALL}"
            )
            self.sample_rate = 48000

        # è®¡ç®—å¸§å¤§å°å’Œåˆ†æ®µå‚æ•°
        self.frame_size = int(self.sample_rate * self.frame_duration / 1000)
        frames_per_second = 1000 / self.frame_duration
        self.min_segment_frames = int(self.min_segment_duration * frames_per_second)
        self.max_segment_frames = int(self.max_segment_duration * frames_per_second)

        print(f"{Fore.GREEN}âœ“ éŸ³é¢‘å¸§å¤§å°: {self.frame_size} æ ·æœ¬{Style.RESET_ALL}")
        print(
            f"{Fore.GREEN}âœ“ åˆ†æ®µå‚æ•°: {self.min_segment_duration}s-{self.max_segment_duration}s{Style.RESET_ALL}"
        )

        # æ£€æŸ¥Multi-Output Deviceé…ç½®
        if multi_output_id is not None:
            print(
                f"{Fore.GREEN}âœ“ å‘ç°Multi-Output Device: [{multi_output_id}] {devices[multi_output_id]['name']}{Style.RESET_ALL}"
            )
            print(
                f"{Fore.CYAN}ğŸ’¡ å»ºè®®å°†ç³»ç»Ÿè¾“å‡ºè®¾ç½®ä¸ºMulti-Output Device{Style.RESET_ALL}"
            )
            print(
                f"{Fore.CYAN}   è¿™æ ·éŸ³é¢‘ä¼šåŒæ—¶è¾“å‡ºåˆ°Speakerså’ŒBlackHole{Style.RESET_ALL}"
            )

        print()

    def setup_vad(self):
        """è®¾ç½®WebRTC VAD"""
        try:
            # åˆ›å»ºVADå®ä¾‹ï¼Œæ•æ„Ÿåº¦è®¾ç½®ä¸º2ï¼ˆä¸­ç­‰ï¼‰
            self.vad = webrtcvad.Vad(2)
            print(f"{Fore.GREEN}âœ“ WebRTC VAD åˆå§‹åŒ–æˆåŠŸ{Style.RESET_ALL}")
        except Exception as e:
            print(f"{Fore.RED}âŒ WebRTC VAD åˆå§‹åŒ–å¤±è´¥: {e}{Style.RESET_ALL}")
            print(f"{Fore.YELLOW}è¯·ç¡®ä¿å·²å®‰è£…: pip install webrtcvad{Style.RESET_ALL}")
            sys.exit(1)

    def audio_callback(self, indata, frames, time, status):
        """éŸ³é¢‘å›è°ƒå‡½æ•° - åŸºäºsounddeviceå®˜æ–¹ç¤ºä¾‹"""
        if status:
            print(f"âš ï¸ éŸ³é¢‘çŠ¶æ€: {status}")
        if self.listening:
            self.audio_queue.put(indata.copy())

    def detect_speech(self, audio_chunk):
        """ä½¿ç”¨WebRTC VADè¿›è¡Œè¯­éŸ³æ´»åŠ¨æ£€æµ‹"""
        try:
            # WebRTC VADæ”¯æŒçš„é‡‡æ ·ç‡å’Œå¯¹åº”çš„å¸§é•¿åº¦
            vad_configs = {
                8000: 80,  # 10ms @ 8kHz
                16000: 160,  # 10ms @ 16kHz
                32000: 320,  # 10ms @ 32kHz
                48000: 480,  # 10ms @ 48kHz
            }

            # é€‰æ‹©æœ€æ¥è¿‘çš„é‡‡æ ·ç‡
            if self.sample_rate in vad_configs:
                vad_sample_rate = self.sample_rate
                vad_frame_size = vad_configs[vad_sample_rate]
            else:
                # é‡é‡‡æ ·åˆ°16000Hz
                vad_sample_rate = 16000
                vad_frame_size = vad_configs[vad_sample_rate]

                # è®¡ç®—é‡é‡‡æ ·å› å­
                downsample_factor = self.sample_rate // vad_sample_rate
                if downsample_factor > 1:
                    audio_chunk = audio_chunk[::downsample_factor]
                else:
                    # å¦‚æœé‡‡æ ·ç‡å¤ªä½ï¼Œè¿›è¡Œä¸Šé‡‡æ ·
                    upsample_factor = vad_sample_rate // self.sample_rate
                    audio_chunk = np.repeat(audio_chunk, upsample_factor)

            # ç¡®ä¿éŸ³é¢‘é•¿åº¦æ˜¯VADå¸§å¤§å°çš„æ•´æ•°å€
            if len(audio_chunk) < vad_frame_size:
                # å¦‚æœå¤ªçŸ­ï¼Œç”¨é›¶å¡«å……
                audio_chunk = np.pad(
                    audio_chunk, (0, vad_frame_size - len(audio_chunk))
                )
            elif len(audio_chunk) > vad_frame_size:
                # å¦‚æœå¤ªé•¿ï¼Œæˆªå–åˆ°æœ€è¿‘çš„å¸§è¾¹ç•Œ
                num_frames = len(audio_chunk) // vad_frame_size
                audio_chunk = audio_chunk[: num_frames * vad_frame_size]

            # è½¬æ¢ä¸ºint16æ ¼å¼ï¼Œç¡®ä¿èŒƒå›´æ­£ç¡®
            audio_float = np.clip(audio_chunk, -1.0, 1.0)
            audio_int16 = (audio_float * 32767).astype(np.int16)

            # ä½¿ç”¨WebRTC VADæ£€æµ‹è¯­éŸ³
            return self.vad.is_speech(audio_int16.tobytes(), vad_sample_rate)

        except Exception as e:
            print(f"VADé”™è¯¯: {e}")
            # VADå¤±è´¥æ—¶ï¼Œå›é€€åˆ°ç®€å•çš„èƒ½é‡æ£€æµ‹
            try:
                rms = np.sqrt(np.mean(audio_chunk**2))
                threshold = 0.001
                return rms > threshold
            except:
                return False

    def save_audio_segment(self, audio_data):
        """ä¿å­˜éŸ³é¢‘ç‰‡æ®µä¸ºä¸´æ—¶æ–‡ä»¶"""
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as f:
            audio_int16 = (audio_data * 32767).astype(np.int16)

            with wave.open(f.name, "wb") as wav_file:
                wav_file.setnchannels(1)
                wav_file.setsampwidth(2)
                wav_file.setframerate(self.sample_rate)
                wav_file.writeframes(audio_int16.tobytes())

            return f.name

    def transcribe_with_whisper(self, audio_file):
        """ä½¿ç”¨Whisperè¿›è¡Œè¯­éŸ³è¯†åˆ«"""
        try:
            # æ„å»ºwhisper-cliå‘½ä»¤
            cmd = [
                "whisper-cli",
                "-m",
                self.whisper_model_path,
                "-f",
                audio_file,
                "--output-json",
                "--language",
                "auto",
                "--no-timestamps",
                "--threads",
                "4",
            ]

            print(cmd)
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=15)

            if result.returncode == 0:
                # å°è¯•è§£æJSONè¾“å‡º
                json_file = audio_file.replace(".wav", ".json")

                if os.path.exists(json_file):
                    try:
                        with open(json_file, "r") as f:
                            data = json.load(f)
                            if "transcription" in data and data["transcription"]:
                                text = data["transcription"][0]["text"].strip()
                                if text and len(text) > 3:
                                    return text
                        os.unlink(json_file)
                    except Exception as e:
                        pass

                # Fallback: è§£ææ ‡å‡†è¾“å‡º
                if result.stdout:
                    lines = result.stdout.strip().split("\n")
                    for line in lines:
                        line = line.strip()
                        if (
                            line
                            and len(line) > 3
                            and not line.startswith("[")
                            and "whisper" not in line.lower()
                        ):
                            return line

            return None

        except Exception as e:
            print(f"âŒ Whisperè½¬å½•å¤±è´¥: {e}")
            return None

    def process_audio_segment(self, audio_data, segment_id):
        """å¤„ç†éŸ³é¢‘ç‰‡æ®µ"""
        try:
            start_time = time.time()

            # ä¿å­˜éŸ³é¢‘æ–‡ä»¶
            audio_file = self.save_audio_segment(audio_data)

            # ä½¿ç”¨Whisperè½¬å½•
            transcription = self.transcribe_with_whisper(audio_file)

            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            try:
                os.unlink(audio_file)
            except:
                pass

            if transcription:
                processing_time = time.time() - start_time

                # è®°å½•è½¬å½•ç»“æœ
                result = {
                    "segment_id": segment_id,
                    "transcription": transcription,
                    "duration": len(audio_data) / self.sample_rate,
                    "processing_time": processing_time,
                    "timestamp": time.strftime("%H:%M:%S"),
                }

                self.transcription_history.append(result)

                # æ˜¾ç¤ºç»“æœ
                print(
                    f"\n{Fore.GREEN}ğŸ“ ç‰‡æ®µ {segment_id} ({result['duration']:.1f}s):{Style.RESET_ALL}"
                )
                print(f"{Fore.CYAN}{transcription}{Style.RESET_ALL}")
                print(f"{Fore.YELLOW}å¤„ç†æ—¶é—´: {processing_time:.2f}s{Style.RESET_ALL}")
            else:
                print(f"\n{Fore.RED}âŒ ç‰‡æ®µ {segment_id} è½¬å½•å¤±è´¥{Style.RESET_ALL}")

        except Exception as e:
            print(f"âŒ å¤„ç†éŸ³é¢‘ç‰‡æ®µå¤±è´¥: {e}")

    def start_transcription(self):
        """å¼€å§‹è½¬å½•"""
        print(f"{Fore.CYAN}ğŸ¤ å¼€å§‹éŸ³é¢‘è½¬å½•...{Style.RESET_ALL}")
        print(f"{Fore.YELLOW}ğŸ’¡ è¯·æ’­æ”¾éŸ³é¢‘æˆ–è¯´è¯...{Style.RESET_ALL}")
        print(f"{Fore.YELLOW}ğŸ’¡ æŒ‰ Ctrl+C åœæ­¢è½¬å½•{Style.RESET_ALL}")
        print()

        # éŸ³é¢‘ç¼“å†²
        speech_frames = []
        silence_count = 0

        self.listening = True

        try:
            # å¯åŠ¨éŸ³é¢‘æµ - åŸºäºsounddeviceå®˜æ–¹ç¤ºä¾‹
            with sd.InputStream(
                device=self.audio_device,
                samplerate=self.sample_rate,
                channels=1,
                dtype=np.float32,
                blocksize=self.frame_size,
                callback=self.audio_callback,
            ):
                print(f"{Fore.GREEN}ğŸ§ å¼€å§‹ç›‘å¬éŸ³é¢‘...{Style.RESET_ALL}")

                while True:
                    try:
                        # ä»é˜Ÿåˆ—è·å–éŸ³é¢‘æ•°æ®
                        audio_chunk = self.audio_queue.get(timeout=0.1)
                        is_speech = self.detect_speech(audio_chunk.flatten())

                        if is_speech:
                            speech_frames.append(audio_chunk.flatten())
                            silence_count = 0

                            # æ˜¾ç¤ºè¿›åº¦
                            if len(speech_frames) % 10 == 0:  # æ›´é¢‘ç¹çš„è¿›åº¦æ˜¾ç¤º
                                duration = (
                                    len(speech_frames) * self.frame_duration / 1000
                                )
                                print(
                                    f"\r{Fore.CYAN}ğŸ—£ï¸ å½•éŸ³ä¸­: {duration:.1f}s{Style.RESET_ALL}",
                                    end="",
                                    flush=True,
                                )
                        else:
                            silence_count += 1

                        # åˆ¤æ–­æ˜¯å¦å¤„ç†ç‰‡æ®µ
                        should_process = False

                        if speech_frames:
                            # è¾¾åˆ°æœ€å°æ—¶é•¿ä¸”æœ‰è¶³å¤Ÿé™éŸ³
                            if (
                                len(speech_frames) >= self.min_segment_frames
                                and silence_count >= self.silence_threshold
                            ):
                                should_process = True
                            # æˆ–è€…è¾¾åˆ°æœ€å¤§æ—¶é•¿
                            elif len(speech_frames) >= self.max_segment_frames:
                                should_process = True

                        if should_process:
                            duration = len(speech_frames) * self.frame_duration / 1000
                            print(
                                f"\r{Fore.GREEN}ğŸ“ å¤„ç†ç‰‡æ®µ ({duration:.1f}s)...{Style.RESET_ALL}"
                            )

                            full_audio = np.concatenate(speech_frames)
                            self.segment_counter += 1

                            # å¼‚æ­¥å¤„ç†
                            threading.Thread(
                                target=self.process_audio_segment,
                                args=(full_audio, self.segment_counter),
                                daemon=True,
                            ).start()

                            # é‡ç½®
                            speech_frames = []
                            silence_count = 0

                    except queue.Empty:
                        continue

        except KeyboardInterrupt:
            print(f"\n{Fore.CYAN}ğŸ›‘ åœæ­¢è½¬å½•...{Style.RESET_ALL}")
            self.listening = False
            time.sleep(1)
            print(f"{Fore.GREEN}ğŸ‘‹ è½¬å½•ç»“æŸï¼{Style.RESET_ALL}")

            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            if self.transcription_history:
                total_segments = len(self.transcription_history)
                avg_time = (
                    sum(item["processing_time"] for item in self.transcription_history)
                    / total_segments
                )
                print(
                    f"{Fore.YELLOW}ğŸ“Š ç»Ÿè®¡: å…±å¤„ç† {total_segments} ä¸ªç‰‡æ®µï¼Œå¹³å‡å¤„ç†æ—¶é—´ {avg_time:.2f}s{Style.RESET_ALL}"
                )


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ ç®€åŒ–ç‰ˆå®æ—¶è½¬å½•ç³»ç»Ÿ")
    print("=" * 40)

    # åˆ›å»ºè½¬å½•å™¨
    transcriber = SimpleTranscriber(whisper_model="small")

    # å¼€å§‹è½¬å½•
    transcriber.start_transcription()


if __name__ == "__main__":
    main()
